{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6ef29a",
   "metadata": {},
   "source": [
    "# Ambil dataset dari roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install roboflow\n",
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252636e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in tennis-ball-detection-6 to yolov8:: 100%|██████████| 52040/52040 [01:04<00:00, 807.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to tennis-ball-detection-6 in yolov8:: 100%|██████████| 1168/1168 [00:03<00:00, 312.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"M4ADE509JQ3BwLY9kHR7\")\n",
    "project = rf.workspace(\"viren-dhanwani\").project(\"tennis-ball-detection\")\n",
    "version = project.version(6)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f4930",
   "metadata": {},
   "source": [
    "# TRAINING DI GOOGLE COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bersih & pasang versi yang cocok\n",
    "# !pip uninstall -y torch torchvision torchaudio ultralytics\n",
    "# !pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install ultralytics==8.3.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"M4ADE509JQ3BwLY9kHR7\")\n",
    "# project = rf.workspace(\"viren-dhanwani\").project(\"tennis-ball-detection\")\n",
    "# version = project.version(6)\n",
    "# dataset = version.download(\"yolov8\")\n",
    "\n",
    "# !yolo task=detect mode=train \\\n",
    "# model=yolov8s.yaml pretrained=yolov8s.pt \\\n",
    "#   data={dataset.location}/data.yaml \\\n",
    "#   epochs=100 imgsz=640 batch=-1 patience=20 \\\n",
    "#   resume=False project=runs name=tennis_v8s_v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7230185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tennis-ball-detection-6/tennis-ball-detection-6/valid'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.move(\"tennis-ball-detection-6/train\", \n",
    "\"tennis-ball-detection-6/tennis-ball-detection-6/train\",\n",
    ")\n",
    "\n",
    "shutil.move(\"tennis-ball-detection-6/test\", \n",
    "\"tennis-ball-detection-6/tennis-ball-detection-6/test\",\n",
    ")\n",
    "\n",
    "shutil.move(\"tennis-ball-detection-6/valid\", \n",
    "\"tennis-ball-detection-6/tennis-ball-detection-6/valid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1040593",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train model=yolov8s.pt data={dataset.location}/data.yaml epochs=100 imgsz=640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af61a50",
   "metadata": {},
   "source": [
    "# IMPROVED TRAINING - Consistent Detection Across All Videos\n",
    "\n",
    "## Strategy:\n",
    "1. **Stratified Split** (75-15-10) berdasarkan ball size\n",
    "2. **Heavy Augmentation** untuk generalisasi\n",
    "3. **Extended Epochs** (150) dengan early stopping\n",
    "4. **Optimizer AdamW** untuk better convergence\n",
    "5. **Reproducible** dengan seed=42\n",
    "\n",
    "## Expected Results:\n",
    "- Detection rate: 70-80% (consistent across videos)\n",
    "- mAP@50: >75%\n",
    "- Recall: >70%\n",
    "- Low false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b233f635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (1.2.9)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: certifi in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (2025.8.3)\n",
      "Requirement already satisfied: idna==3.7 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (1.4.9)\n",
      "Requirement already satisfied: matplotlib in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (3.10.6)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (1.26.4)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (11.3.0)\n",
      "Requirement already satisfied: pi-heif<2 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (1.1.0)\n",
      "Requirement already satisfied: pillow-avif-plugin<2 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (1.1.1)\n",
      "Requirement already satisfied: requests in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (2.32.5)\n",
      "Requirement already satisfied: six in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (2.5.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (6.0.3)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib->roboflow) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib->roboflow) (4.60.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib->roboflow) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib->roboflow) (3.2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from requests->roboflow) (3.4.3)\n",
      "Requirement already satisfied: ultralytics in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (8.3.203)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (3.10.6)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (4.9.0.80)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (0.18.1+cu121)\n",
      "Requirement already satisfied: psutil in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (7.1.0)\n",
      "Requirement already satisfied: polars in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (1.33.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (2.0.17)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: filelock in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: sympy in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ultralytics in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (8.3.203)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (3.10.6)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (4.9.0.80)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (0.18.1+cu121)\n",
      "Requirement already satisfied: psutil in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (7.1.0)\n",
      "Requirement already satisfied: polars in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (1.33.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from ultralytics) (2.0.17)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: filelock in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: sympy in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\kuliah\\skripsi\\tennis_analysis\\.venv\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install roboflow\n",
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5160f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in tennis-ball-detection-6 to yolov8:: 100%|██████████| 52040/52040 [00:29<00:00, 1735.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to tennis-ball-detection-6 in yolov8:: 100%|██████████| 1168/1168 [00:00<00:00, 2048.73it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"M4ADE509JQ3BwLY9kHR7\")\n",
    "project = rf.workspace(\"viren-dhanwani\").project(\"tennis-ball-detection\")\n",
    "version = project.version(6)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbdc754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "📂 DATASET LOCATION VERIFICATION\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📂 DATASET LOCATION VERIFICATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBall Detection Dataset:    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected:                  training/tennis-ball-detection-6/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ This is SEPARATE from Court Keypoints dataset!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Verify Dataset Location (IMPORTANT!)\n",
    "print(\"=\"*70)\n",
    "print(\"📂 DATASET LOCATION VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Ball Detection Dataset:    {dataset.location}\")\n",
    "print(f\"Expected:                  training/tennis-ball-detection-6/\")\n",
    "print(\"\\n✅ This is SEPARATE from Court Keypoints dataset!\")\n",
    "print(\"   Court Keypoints:        training/Court-Keypoints/\")\n",
    "print(\"   Ball Detection:         training/tennis-ball-detection-6/\")\n",
    "print(\"\\n✅ NO CONFLICT - Different folders, different models\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e65d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🔍 VERIFYING SPLIT RESULTS\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔍 VERIFYING SPLIT RESULTS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m Path(\u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mlocation)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     11\u001b[0m     images_path \u001b[38;5;241m=\u001b[39m dataset_path \u001b[38;5;241m/\u001b[39m split_name \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Verify Split Results\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🔍 VERIFYING SPLIT RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dataset_path = Path(dataset.location)\n",
    "\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    images_path = dataset_path / split_name / 'images'\n",
    "    labels_path = dataset_path / split_name / 'labels'\n",
    "    \n",
    "    if images_path.exists():\n",
    "        num_images = len(list(images_path.glob('*.jpg'))) + len(list(images_path.glob('*.png')))\n",
    "        num_labels = len(list(labels_path.glob('*.txt')))\n",
    "        \n",
    "        print(f\"\\n{split_name.upper()}:\")\n",
    "        print(f\"  Images: {num_images}\")\n",
    "        print(f\"  Labels: {num_labels}\")\n",
    "        print(f\"  Match:  {'✅' if num_images == num_labels else '❌'}\")\n",
    "\n",
    "# Calculate percentages\n",
    "train_count = len(list((dataset_path / 'train' / 'images').glob('*.jpg'))) + len(list((dataset_path / 'train' / 'images').glob('*.png')))\n",
    "val_count = len(list((dataset_path / 'valid' / 'images').glob('*.jpg'))) + len(list((dataset_path / 'valid' / 'images').glob('*.png')))\n",
    "test_count = len(list((dataset_path / 'test' / 'images').glob('*.jpg'))) + len(list((dataset_path / 'test' / 'images').glob('*.png')))\n",
    "\n",
    "total = train_count + val_count + test_count\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"📊 FINAL PERCENTAGES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total:      {total} images\")\n",
    "print(f\"Train:      {train_count} ({train_count/total*100:.2f}%)\")\n",
    "print(f\"Validation: {val_count} ({val_count/total*100:.2f}%)\")\n",
    "print(f\"Test:       {test_count} ({test_count/total*100:.2f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if meets 75-15-10 target\n",
    "train_pct = train_count/total*100\n",
    "val_pct = val_count/total*100\n",
    "test_pct = test_count/total*100\n",
    "\n",
    "if abs(train_pct - 75.0) < 1.0 and abs(val_pct - 15.0) < 1.0 and abs(test_pct - 10.0) < 1.0:\n",
    "    print(\"✅ SUCCESS! Split meets 75-15-10 target (±1%)\")\n",
    "else:\n",
    "    print(\"⚠️  Split deviates from 75-15-10 target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187acbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🚀 TRAINING IMPROVED MODEL\n",
      "======================================================================\n",
      "Configuration:\n",
      "  - Model: YOLOv8s\n",
      "  - Epochs: 150 (with early stopping patience=30)\n",
      "  - Optimizer: AdamW\n",
      "  - Heavy Augmentation for generalization\n",
      "  - Seed: 42 (reproducible)\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train with improved config\u001b[39;00m\n\u001b[0;32m     19\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m---> 20\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/data.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,              \u001b[38;5;66;03m# ✅ Increased for better convergence\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m,\n\u001b[0;32m     23\u001b[0m     batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,                \u001b[38;5;66;03m# Adjust based on GPU memory\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,             \u001b[38;5;66;03m# Early stopping\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Optimizer - AdamW better than SGD for small datasets\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdamW\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m     lr0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,               \u001b[38;5;66;03m# Initial learning rate\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     lrf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,                \u001b[38;5;66;03m# Final LR (lr0 * lrf)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.937\u001b[39m,\n\u001b[0;32m     31\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Heavy Augmentation for generalization across videos\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m     hsv_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.015\u001b[39m,             \u001b[38;5;66;03m# Hue (lighting variation)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     hsv_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,               \u001b[38;5;66;03m# Saturation (court color)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     hsv_v\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m,               \u001b[38;5;66;03m# Value (brightness)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     degrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,               \u001b[38;5;66;03m# Rotation ±5°\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     translate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,           \u001b[38;5;66;03m# Translation 10%\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,               \u001b[38;5;66;03m# Scale variation 30%\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     flipud\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,              \u001b[38;5;66;03m# NO vertical flip (physics!)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     fliplr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,              \u001b[38;5;66;03m# 50% horizontal flip\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     mosaic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,              \u001b[38;5;66;03m# Mosaic augmentation\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     mixup\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,               \u001b[38;5;66;03m# Mixup for better generalization\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Loss weights\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     box\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7.5\u001b[39m,                 \u001b[38;5;66;03m# Box loss weight\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,                 \u001b[38;5;66;03m# Classification loss\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Other settings\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     cos_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,             \u001b[38;5;66;03m# Cosine LR scheduler\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     close_mosaic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,         \u001b[38;5;66;03m# Disable mosaic last 10 epochs\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,                \u001b[38;5;66;03m# GPU 0\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     55\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/detect\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     56\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtennis_ball_improved_v6\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     57\u001b[0m     exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m     pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     59\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m                  \u001b[38;5;66;03m# ✅ Reproducibility for thesis\u001b[39;00m\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ TRAINING COMPLETE!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model saved at: runs/detect/tennis_ball_improved_v6/weights/best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2: Train with IMPROVED Configuration\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🚀 TRAINING IMPROVED MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if dataset variable exists\n",
    "try:\n",
    "    dataset_path = dataset.location\n",
    "    print(f\"✅ Dataset found at: {dataset_path}\")\n",
    "except NameError:\n",
    "    print(\"❌ ERROR: 'dataset' variable not defined!\")\n",
    "    print(\"\\n📋 Required steps BEFORE training:\")\n",
    "    print(\"1. Run cell: 'from roboflow import Roboflow' (download dataset)\")\n",
    "    print(\"2. Verify dataset downloaded\")\n",
    "    print(\"3. Then run this training cell\")\n",
    "    print(\"\\n⚠️  Please run the dataset download cell first!\")\n",
    "    raise NameError(\"Dataset not loaded. Run Roboflow download cell first.\")\n",
    "\n",
    "# Verify dataset path exists\n",
    "if not Path(dataset_path).exists():\n",
    "    print(f\"❌ ERROR: Dataset path not found: {dataset_path}\")\n",
    "    raise FileNotFoundError(f\"Dataset path does not exist: {dataset_path}\")\n",
    "\n",
    "# Verify data.yaml exists\n",
    "data_yaml = Path(dataset_path) / 'data.yaml'\n",
    "if not data_yaml.exists():\n",
    "    print(f\"❌ ERROR: data.yaml not found at {data_yaml}\")\n",
    "    raise FileNotFoundError(f\"data.yaml not found: {data_yaml}\")\n",
    "\n",
    "print(\"✅ Dataset path verified\")\n",
    "print(f\"✅ data.yaml found: {data_yaml}\")\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"  - Model: YOLOv8s\")\n",
    "print(\"  - Epochs: 150 (with early stopping patience=30)\")\n",
    "print(\"  - Optimizer: AdamW\")\n",
    "print(\"  - Heavy Augmentation for generalization\")\n",
    "print(\"  - Seed: 42 (reproducible)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\n📦 Loading YOLOv8s pretrained model...\")\n",
    "model = YOLO('yolov8s.pt')\n",
    "print(\"✅ Model loaded\")\n",
    "\n",
    "# Train with improved config\n",
    "print(\"\\n🎯 Starting training...\")\n",
    "print(\"⏰ This will take a while (depends on GPU/CPU)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "results = model.train(\n",
    "    data=str(data_yaml),\n",
    "    epochs=150,              # ✅ Increased for better convergence\n",
    "    imgsz=640,\n",
    "    batch=16,                # Adjust based on GPU memory\n",
    "    patience=30,             # Early stopping\n",
    "    \n",
    "    # Optimizer - AdamW better than SGD for small datasets\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,               # Initial learning rate\n",
    "    lrf=0.01,                # Final LR (lr0 * lrf)\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    \n",
    "    # Heavy Augmentation for generalization across videos\n",
    "    augment=True,\n",
    "    hsv_h=0.015,             # Hue (lighting variation)\n",
    "    hsv_s=0.7,               # Saturation (court color)\n",
    "    hsv_v=0.4,               # Value (brightness)\n",
    "    degrees=5,               # Rotation ±5°\n",
    "    translate=0.1,           # Translation 10%\n",
    "    scale=0.3,               # Scale variation 30%\n",
    "    flipud=0.0,              # NO vertical flip (physics!)\n",
    "    fliplr=0.5,              # 50% horizontal flip\n",
    "    mosaic=1.0,              # Mosaic augmentation\n",
    "    mixup=0.1,               # Mixup for better generalization\n",
    "    \n",
    "    # Loss weights\n",
    "    box=7.5,                 # Box loss weight\n",
    "    cls=0.5,                 # Classification loss\n",
    "    \n",
    "    # Other settings\n",
    "    cos_lr=True,             # Cosine LR scheduler\n",
    "    close_mosaic=10,         # Disable mosaic last 10 epochs\n",
    "    device=0,                # GPU 0 (change to 'cpu' if no GPU)\n",
    "    workers=8,\n",
    "    project='runs/detect',\n",
    "    name='tennis_ball_improved_v6',\n",
    "    exist_ok=False,\n",
    "    pretrained=True,\n",
    "    verbose=True,\n",
    "    seed=42                  # ✅ Reproducibility for thesis\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"📁 Best model: runs/detect/tennis_ball_improved_v6/weights/best.pt\")\n",
    "print(f\"📁 Last model: runs/detect/tennis_ball_improved_v6/weights/last.pt\")\n",
    "print(f\"📊 Results: runs/detect/tennis_ball_improved_v6/\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ee7a1",
   "metadata": {},
   "source": [
    "# ⚠️ BEFORE TRAINING - CHECKLIST\n",
    "\n",
    "## Prerequisites:\n",
    "\n",
    "Sebelum menjalankan cell training, pastikan sudah menjalankan:\n",
    "\n",
    "### ✅ 1. Dataset Downloaded\n",
    "Run cell ini terlebih dahulu:\n",
    "```python\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"M4ADE509JQ3BwLY9kHR7\")\n",
    "project = rf.workspace(\"viren-dhanwani\").project(\"tennis-ball-detection\")\n",
    "version = project.version(6)\n",
    "dataset = version.download(\"yolov8\")\n",
    "```\n",
    "\n",
    "### ✅ 2. Dataset Split Verified\n",
    "Run verification cell untuk memastikan split OK:\n",
    "- Train: ~75%\n",
    "- Valid: ~15%\n",
    "- Test: ~10%\n",
    "\n",
    "### ✅ 3. GPU/CPU Ready\n",
    "Check:\n",
    "- GPU available: Faster training (~2-4 hours)\n",
    "- CPU only: Slower training (~12-24 hours)\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check:\n",
    "\n",
    "Run cell berikut untuk verify semua ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee232c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-TRAINING CHECKLIST - Run this first!\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🔍 PRE-TRAINING CHECKLIST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check 1: Dataset variable\n",
    "print(\"\\n1️⃣ Checking dataset variable...\")\n",
    "try:\n",
    "    dataset_path = dataset.location\n",
    "    print(f\"   ✅ Dataset variable exists: {dataset_path}\")\n",
    "    \n",
    "    # Verify path exists\n",
    "    if Path(dataset_path).exists():\n",
    "        print(f\"   ✅ Dataset path exists\")\n",
    "    else:\n",
    "        print(f\"   ❌ Dataset path NOT found: {dataset_path}\")\n",
    "        print(\"   👉 Re-run dataset download cell!\")\n",
    "        \n",
    "except NameError:\n",
    "    print(\"   ❌ Dataset variable NOT defined\")\n",
    "    print(\"   👉 Run the Roboflow download cell first!\")\n",
    "    print(\"\\n   Required cell:\")\n",
    "    print(\"   from roboflow import Roboflow\")\n",
    "    print(\"   rf = Roboflow(api_key='...')\")\n",
    "    print(\"   ...\")\n",
    "    print(\"   dataset = version.download('yolov8')\")\n",
    "\n",
    "# Check 2: Data splits\n",
    "print(\"\\n2️⃣ Checking data splits...\")\n",
    "try:\n",
    "    dataset_root = Path(dataset.location)\n",
    "    \n",
    "    train_imgs = len(list((dataset_root / 'train' / 'images').glob('*.jpg'))) + \\\n",
    "                 len(list((dataset_root / 'train' / 'images').glob('*.png')))\n",
    "    val_imgs = len(list((dataset_root / 'valid' / 'images').glob('*.jpg'))) + \\\n",
    "               len(list((dataset_root / 'valid' / 'images').glob('*.png')))\n",
    "    test_imgs = len(list((dataset_root / 'test' / 'images').glob('*.jpg'))) + \\\n",
    "                len(list((dataset_root / 'test' / 'images').glob('*.png')))\n",
    "    \n",
    "    total = train_imgs + val_imgs + test_imgs\n",
    "    \n",
    "    if total > 0:\n",
    "        print(f\"   ✅ Train: {train_imgs} images ({train_imgs/total*100:.1f}%)\")\n",
    "        print(f\"   ✅ Valid: {val_imgs} images ({val_imgs/total*100:.1f}%)\")\n",
    "        print(f\"   ✅ Test:  {test_imgs} images ({test_imgs/total*100:.1f}%)\")\n",
    "        print(f\"   ✅ Total: {total} images\")\n",
    "    else:\n",
    "        print(\"   ❌ No images found in splits!\")\n",
    "        print(\"   👉 Run split cells or check dataset\")\n",
    "except:\n",
    "    print(\"   ⚠️  Could not check splits (dataset not loaded)\")\n",
    "\n",
    "# Check 3: data.yaml\n",
    "print(\"\\n3️⃣ Checking data.yaml...\")\n",
    "try:\n",
    "    data_yaml = Path(dataset.location) / 'data.yaml'\n",
    "    if data_yaml.exists():\n",
    "        print(f\"   ✅ data.yaml exists: {data_yaml}\")\n",
    "    else:\n",
    "        print(f\"   ❌ data.yaml NOT found: {data_yaml}\")\n",
    "        print(\"   👉 Check dataset download or run split\")\n",
    "except:\n",
    "    print(\"   ⚠️  Could not check data.yaml\")\n",
    "\n",
    "# Check 4: GPU availability\n",
    "print(\"\\n4️⃣ Checking GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"   ✅ GPU available: {gpu_name}\")\n",
    "    print(f\"   ✅ GPU memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   ⚡ Training will be FAST (~2-4 hours)\")\n",
    "else:\n",
    "    print(\"   ⚠️  No GPU detected\")\n",
    "    print(\"   💻 Will use CPU (training will be SLOW ~12-24 hours)\")\n",
    "    print(\"   💡 Consider using Google Colab for GPU training\")\n",
    "\n",
    "# Check 5: Ultralytics installed\n",
    "print(\"\\n5️⃣ Checking ultralytics...\")\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    import ultralytics\n",
    "    print(f\"   ✅ Ultralytics installed: v{ultralytics.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"   ❌ Ultralytics NOT installed\")\n",
    "    print(\"   👉 Run: %pip install ultralytics\")\n",
    "\n",
    "# Final verdict\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📋 FINAL VERDICT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    if Path(dataset.location).exists() and total > 0:\n",
    "        print(\"✅ ALL CHECKS PASSED - Ready for training!\")\n",
    "        print(\"\\n👉 Run the next cell to start training\")\n",
    "    else:\n",
    "        print(\"❌ Some checks failed - fix issues above first\")\n",
    "except:\n",
    "    print(\"❌ Dataset not loaded - run Roboflow download cell first!\")\n",
    "    \n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Evaluate on Test Set\n",
    "print(\"=\"*70)\n",
    "print(\"📊 EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "best_model = YOLO('runs/detect/tennis_ball_improved_v6/weights/best.pt')\n",
    "\n",
    "# Validate on test set\n",
    "test_results = best_model.val(\n",
    "    data=f'{dataset.location}/data.yaml',\n",
    "    split='test',\n",
    "    batch=16,\n",
    "    imgsz=640,\n",
    "    device=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎯 TEST SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"mAP@50:        {test_results.box.map50:.4f} (target: >0.75)\")\n",
    "print(f\"mAP@50-95:     {test_results.box.map:.4f} (target: >0.35)\")\n",
    "print(f\"Precision:     {test_results.box.mp:.4f} (target: >0.85)\")\n",
    "print(f\"Recall:        {test_results.box.mr:.4f} (target: >0.70)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluation\n",
    "if test_results.box.map50 > 0.75 and test_results.box.mr > 0.70:\n",
    "    print(\"✅ EXCELLENT! Model exceeds all targets\")\n",
    "elif test_results.box.map50 > 0.65 and test_results.box.mr > 0.60:\n",
    "    print(\"✅ GOOD! Model meets acceptable thresholds\")\n",
    "else:\n",
    "    print(\"⚠️  Model needs more training or data augmentation\")\n",
    "\n",
    "# Save results to JSON for thesis documentation\n",
    "import json\n",
    "results_dict = {\n",
    "    'model': 'yolov8s_improved_v6',\n",
    "    'test_set_results': {\n",
    "        'mAP@50': float(test_results.box.map50),\n",
    "        'mAP@50-95': float(test_results.box.map),\n",
    "        'precision': float(test_results.box.mp),\n",
    "        'recall': float(test_results.box.mr)\n",
    "    },\n",
    "    'training_config': {\n",
    "        'epochs': 150,\n",
    "        'optimizer': 'AdamW',\n",
    "        'augmentation': 'heavy',\n",
    "        'split': '75-15-10',\n",
    "        'seed': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('runs/detect/tennis_ball_improved_v6/test_results.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "print(\"\\n💾 Results saved to: runs/detect/tennis_ball_improved_v6/test_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4336bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test Consistency Across Multiple Videos\n",
    "# (Run this after downloading the model to your local machine)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🎬 TESTING CONSISTENCY ACROSS VIDEOS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAfter training, test the model on multiple videos:\")\n",
    "print(\"1. Download best.pt from runs/detect/tennis_ball_improved_v6/weights/\")\n",
    "print(\"2. Rename to yolo8_improved_v6.pt\")\n",
    "print(\"3. Place in models/ folder\")\n",
    "print(\"4. Run consistency test:\\n\")\n",
    "\n",
    "test_code = \"\"\"\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "model = YOLO('models/yolo8_improved_v6.pt')\n",
    "\n",
    "videos = ['input_video1.mp4', 'input_video2.mp4', 'input_video3.mp4']\n",
    "thresholds = [0.10, 0.15, 0.20, 0.25]\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for video_name in videos:\n",
    "    video_path = f'input_videos/{video_name}'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Test first 50 frames\n",
    "    frames = []\n",
    "    for _ in range(50):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    \n",
    "    print(f\"\\\\nTesting {video_name} ({len(frames)} frames):\")\n",
    "    print(f\"{'Conf':<8} {'Detections':<12} {'Rate':<10}\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    for conf in thresholds:\n",
    "        detections = 0\n",
    "        for frame in frames:\n",
    "            results = model.predict(frame, conf=conf, verbose=False)\n",
    "            if len(results[0].boxes) > 0:\n",
    "                detections += 1\n",
    "        \n",
    "        rate = (detections / len(frames)) * 100\n",
    "        results_table.append((video_name, conf, rate))\n",
    "        print(f\"{conf:<8.2f} {detections}/{len(frames):<7} {rate:<10.1f}%\")\n",
    "\n",
    "# Find best universal config\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"FINDING BEST UNIVERSAL CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for conf in thresholds:\n",
    "    rates = [r for v, c, r in results_table if c == conf]\n",
    "    avg_rate = sum(rates) / len(rates)\n",
    "    variance = max(rates) - min(rates)\n",
    "    \n",
    "    print(f\"Conf {conf:.2f}: Avg={avg_rate:.1f}%, Variance={variance:.1f}%\")\n",
    "\n",
    "print(\"\\\\n✅ Choose config with highest avg rate and lowest variance\")\n",
    "\"\"\"\n",
    "\n",
    "print(test_code)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📋 SUCCESS CRITERIA:\")\n",
    "print(\"=\"*70)\n",
    "print(\"✅ Detection rate ≥ 70% on ALL videos\")\n",
    "print(\"✅ Variance between videos < 15%\")\n",
    "print(\"✅ Consistent conf threshold across videos\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2118db99",
   "metadata": {},
   "source": [
    "# ⚡ OPTIMIZED STRATIFIED SPLIT - FAST VERSION\n",
    "\n",
    "## Masalah yang Ditemukan:\n",
    "- Split data menggunakan `shutil.copy()` → **SANGAT LAMBAT** (251 menit!)\n",
    "- Tidak ada progress indicator\n",
    "- Tidak ada check apakah sudah split\n",
    "\n",
    "## Solusi:\n",
    "- ✅ Gunakan `shutil.move()` → **100x lebih cepat** (~10 detik!)\n",
    "- ✅ Progress bar dengan `tqdm`\n",
    "- ✅ Check existing split\n",
    "- ✅ Verification step\n",
    "\n",
    "## Target Split:\n",
    "- **Train**: 75% (~321 images)\n",
    "- **Valid**: 15% (~64 images)\n",
    "- **Test**: 10% (~43 images)\n",
    "\n",
    "## Langkah Eksekusi:\n",
    "1. Install tqdm (jika belum)\n",
    "2. Run cell split function\n",
    "3. Verifikasi hasil split\n",
    "4. Lanjut training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install tqdm untuk progress bar\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: OPTIMIZED Stratified Split Function\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "def stratified_split_optimized(dataset_path, train_ratio=0.75, val_ratio=0.15, test_ratio=0.10, seed=42):\n",
    "    \"\"\"\n",
    "    OPTIMIZED: Stratified split dengan MOVE (bukan copy) untuk speed 100x lebih cepat!\n",
    "    \n",
    "    Performance:\n",
    "    - Old (copy): 251 menit ❌\n",
    "    - New (move): ~10 detik ⚡\n",
    "    \n",
    "    Features:\n",
    "    - ✅ Stratified berdasarkan ball size\n",
    "    - ✅ Progress bar (tqdm)\n",
    "    - ✅ Check existing split\n",
    "    - ✅ Verification step\n",
    "    - ✅ 100x faster dengan move!\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"⚡ OPTIMIZED STRATIFIED SPLIT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"🔍 Analyzing dataset at: {dataset_path}\\n\")\n",
    "    \n",
    "    # Paths\n",
    "    dataset_root = Path(dataset_path)\n",
    "    source_images_path = dataset_root / 'train' / 'images'\n",
    "    source_labels_path = dataset_root / 'train' / 'labels'\n",
    "    \n",
    "    if not source_images_path.exists():\n",
    "        print(f\"❌ Error: {source_images_path} not found!\")\n",
    "        return None\n",
    "    \n",
    "    # Check if already split\n",
    "    valid_path = dataset_root / 'valid' / 'images'\n",
    "    if valid_path.exists() and len(list(valid_path.glob('*.jpg'))) > 0:\n",
    "        val_count = len(list(valid_path.glob('*.jpg')))\n",
    "        test_count = len(list((dataset_root / 'test' / 'images').glob('*.jpg')))\n",
    "        \n",
    "        print(\"⚠️  WARNING: Split ALREADY EXISTS!\")\n",
    "        print(f\"   Valid: {val_count} images\")\n",
    "        print(f\"   Test:  {test_count} images\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"OPTIONS:\")\n",
    "        print(\"1. Skip split (RECOMMENDED if already done)\")\n",
    "        print(\"2. Re-split (will move files again)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        response = input(\"\\nContinue with re-split? (y/n): \").strip().lower()\n",
    "        if response != 'y':\n",
    "            print(\"\\n✅ Split cancelled - using existing split\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"\\n⚠️  Re-splitting dataset...\\n\")\n",
    "    \n",
    "    # Get all images\n",
    "    all_images = sorted(source_images_path.glob('*.jpg'))\n",
    "    if len(all_images) == 0:\n",
    "        all_images = sorted(source_images_path.glob('*.png'))\n",
    "    \n",
    "    print(f\"📊 Found {len(all_images)} images in source\\n\")\n",
    "    \n",
    "    # Analyze ball sizes for stratification\n",
    "    print(\"🔍 Step 1/4: Analyzing ball sizes for stratification...\")\n",
    "    image_stats = []\n",
    "    \n",
    "    for img_file in tqdm(all_images, desc=\"Reading labels\", ncols=80):\n",
    "        label_file = source_labels_path / f\"{img_file.stem}.txt\"\n",
    "        if label_file.exists():\n",
    "            with open(label_file) as f:\n",
    "                lines = f.readlines()\n",
    "                if lines:\n",
    "                    # Get ball size (width * height from YOLO format)\n",
    "                    parts = lines[0].split()\n",
    "                    if len(parts) >= 5:\n",
    "                        width = float(parts[3])\n",
    "                        height = float(parts[4])\n",
    "                        size = width * height\n",
    "                        image_stats.append((img_file, label_file, size))\n",
    "    \n",
    "    print(f\"✅ Analyzed {len(image_stats)} images with labels\\n\")\n",
    "    \n",
    "    # Sort by size for stratification\n",
    "    image_stats.sort(key=lambda x: x[2])\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    n = len(image_stats)\n",
    "    train_n = int(n * train_ratio)\n",
    "    val_n = int(n * val_ratio)\n",
    "    test_n = n - train_n - val_n\n",
    "    \n",
    "    print(\"📊 Step 2/4: Calculating split sizes...\")\n",
    "    print(f\"   Train: {train_n} images ({train_ratio*100:.0f}%)\")\n",
    "    print(f\"   Val:   {val_n} images ({val_ratio*100:.0f}%)\")\n",
    "    print(f\"   Test:  {test_n} images ({test_ratio*100:.0f}%)\")\n",
    "    print(f\"   Total: {n} images\\n\")\n",
    "    \n",
    "    # Stratified indices\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    train_idx = indices[:train_n]\n",
    "    val_idx = indices[train_n:train_n + val_n]\n",
    "    test_idx = indices[train_n + val_n:]\n",
    "    \n",
    "    # Create new directories\n",
    "    print(\"📁 Step 3/4: Creating split directories...\")\n",
    "    for split in ['valid', 'test']:  # train already exists\n",
    "        for subdir in ['images', 'labels']:\n",
    "            split_path = dataset_root / split / subdir\n",
    "            split_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"✅ Directories created\\n\")\n",
    "    \n",
    "    # MOVE files (MUCH faster than copy!)\n",
    "    def move_files(idx_list, split_name):\n",
    "        print(f\"📦 Moving files to {split_name}...\")\n",
    "        moved_img = 0\n",
    "        moved_lbl = 0\n",
    "        \n",
    "        for idx in tqdm(idx_list, desc=f\"Moving {split_name}\", ncols=80):\n",
    "            img_file, label_file, _ = image_stats[idx]\n",
    "            \n",
    "            # Move image\n",
    "            dst_img = dataset_root / split_name / 'images' / img_file.name\n",
    "            if not dst_img.exists():\n",
    "                shutil.move(str(img_file), str(dst_img))\n",
    "                moved_img += 1\n",
    "            \n",
    "            # Move label\n",
    "            dst_label = dataset_root / split_name / 'labels' / label_file.name\n",
    "            if not dst_label.exists():\n",
    "                shutil.move(str(label_file), str(dst_label))\n",
    "                moved_lbl += 1\n",
    "        \n",
    "        return moved_img, moved_lbl\n",
    "    \n",
    "    # Move validation and test (train stays in place)\n",
    "    print(\"⚡ Step 4/4: Moving files (FAST with move, not copy)...\")\n",
    "    val_img, val_lbl = move_files(val_idx, 'valid')\n",
    "    test_img, test_lbl = move_files(test_idx, 'test')\n",
    "    \n",
    "    print(f\"\\n✅ FILES MOVED:\")\n",
    "    print(f\"   Valid: {val_img} images, {val_lbl} labels\")\n",
    "    print(f\"   Test:  {test_img} images, {test_lbl} labels\\n\")\n",
    "    \n",
    "    # Verify final counts\n",
    "    print(\"=\"*70)\n",
    "    print(\"🔍 VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    train_count = len(list((dataset_root / 'train' / 'images').glob('*.jpg'))) + \\\n",
    "                  len(list((dataset_root / 'train' / 'images').glob('*.png')))\n",
    "    val_count = len(list((dataset_root / 'valid' / 'images').glob('*.jpg'))) + \\\n",
    "                len(list((dataset_root / 'valid' / 'images').glob('*.png')))\n",
    "    test_count = len(list((dataset_root / 'test' / 'images').glob('*.jpg'))) + \\\n",
    "                 len(list((dataset_root / 'test' / 'images').glob('*.png')))\n",
    "    \n",
    "    total = train_count + val_count + test_count\n",
    "    \n",
    "    print(f\"Final counts:\")\n",
    "    print(f\"   Train: {train_count} images ({train_count/total*100:.2f}%)\")\n",
    "    print(f\"   Val:   {val_count} images ({val_count/total*100:.2f}%)\")\n",
    "    print(f\"   Test:  {test_count} images ({test_count/total*100:.2f}%)\")\n",
    "    print(f\"   Total: {total} images\")\n",
    "    \n",
    "    # Check if meets target\n",
    "    train_pct = train_count/total*100\n",
    "    val_pct = val_count/total*100\n",
    "    test_pct = test_count/total*100\n",
    "    \n",
    "    print(f\"\\nTarget check:\")\n",
    "    if abs(train_pct - 75.0) < 2.0 and abs(val_pct - 15.0) < 2.0 and abs(test_pct - 10.0) < 2.0:\n",
    "        print(\"✅ SUCCESS! Split meets 75-15-10 target (±2%)\")\n",
    "    else:\n",
    "        print(\"⚠️  Split deviates from 75-15-10 target\")\n",
    "    \n",
    "    # Update data.yaml\n",
    "    print(\"\\n📝 Updating data.yaml...\")\n",
    "    data_yaml_path = dataset_root / 'data.yaml'\n",
    "    if data_yaml_path.exists():\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            data_config = yaml.safe_load(f)\n",
    "        \n",
    "        data_config['train'] = str(dataset_root / 'train' / 'images')\n",
    "        data_config['val'] = str(dataset_root / 'valid' / 'images')\n",
    "        data_config['test'] = str(dataset_root / 'test' / 'images')\n",
    "        \n",
    "        with open(data_yaml_path, 'w') as f:\n",
    "            yaml.dump(data_config, f)\n",
    "        \n",
    "        print(\"✅ data.yaml updated\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"✅ SPLIT COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "# Function defined - ready to use!\n",
    "print(\"✅ Function 'stratified_split_optimized' loaded\")\n",
    "print(\"   Run next cell to execute split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: RUN THE OPTIMIZED SPLIT\n",
    "# Expected time: ~5-10 seconds (was 251 minutes with old method!)\n",
    "\n",
    "print(\"🚀 Starting optimized stratified split...\")\n",
    "print(\"⚡ This should take ~5-10 seconds (100x faster than copy method!)\\n\")\n",
    "\n",
    "# Run the split\n",
    "result = stratified_split_optimized(\n",
    "    dataset.location,\n",
    "    train_ratio=0.75,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "if result is not None:\n",
    "    train_idx, val_idx, test_idx = result\n",
    "    print(f\"\\n✅ Split indices generated:\")\n",
    "    print(f\"   Train indices: {len(train_idx)}\")\n",
    "    print(f\"   Val indices: {len(val_idx)}\")\n",
    "    print(f\"   Test indices: {len(test_idx)}\")\n",
    "else:\n",
    "    print(\"\\n⏭️  Skipped - using existing split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c64ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: VERIFY SPLIT RESULTS\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🔍 DETAILED VERIFICATION OF SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dataset_path = Path(dataset.location)\n",
    "\n",
    "# Check each split\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    images_path = dataset_path / split_name / 'images'\n",
    "    labels_path = dataset_path / split_name / 'labels'\n",
    "    \n",
    "    if images_path.exists():\n",
    "        # Count files\n",
    "        num_images = len(list(images_path.glob('*.jpg'))) + len(list(images_path.glob('*.png')))\n",
    "        num_labels = len(list(labels_path.glob('*.txt')))\n",
    "        \n",
    "        print(f\"\\n{split_name.upper()}:\")\n",
    "        print(f\"  📁 Path: {images_path}\")\n",
    "        print(f\"  🖼️  Images: {num_images}\")\n",
    "        print(f\"  🏷️  Labels: {num_labels}\")\n",
    "        print(f\"  ✅ Match: {'YES' if num_images == num_labels else '❌ NO - MISMATCH!'}\")\n",
    "    else:\n",
    "        print(f\"\\n{split_name.upper()}: ❌ NOT FOUND\")\n",
    "\n",
    "# Calculate final percentages\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 FINAL SPLIT PERCENTAGES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_count = len(list((dataset_path / 'train' / 'images').glob('*.jpg'))) + \\\n",
    "              len(list((dataset_path / 'train' / 'images').glob('*.png')))\n",
    "val_count = len(list((dataset_path / 'valid' / 'images').glob('*.jpg'))) + \\\n",
    "            len(list((dataset_path / 'valid' / 'images').glob('*.png')))\n",
    "test_count = len(list((dataset_path / 'test' / 'images').glob('*.jpg'))) + \\\n",
    "             len(list((dataset_path / 'test' / 'images').glob('*.png')))\n",
    "\n",
    "total = train_count + val_count + test_count\n",
    "\n",
    "if total > 0:\n",
    "    train_pct = train_count/total*100\n",
    "    val_pct = val_count/total*100\n",
    "    test_pct = test_count/total*100\n",
    "    \n",
    "    print(f\"\\nTotal Dataset:  {total} images\")\n",
    "    print(f\"\\nTrain:          {train_count:4d} images ({train_pct:5.2f}%) - Target: 75%\")\n",
    "    print(f\"Validation:     {val_count:4d} images ({val_pct:5.2f}%) - Target: 15%\")\n",
    "    print(f\"Test:           {test_count:4d} images ({test_pct:5.2f}%) - Target: 10%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✅ SUCCESS CRITERIA:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check criteria\n",
    "    checks = []\n",
    "    checks.append((\"Train ~75%\", abs(train_pct - 75.0) < 2.0))\n",
    "    checks.append((\"Val ~15%\", abs(val_pct - 15.0) < 2.0))\n",
    "    checks.append((\"Test ~10%\", abs(test_pct - 10.0) < 2.0))\n",
    "    checks.append((\"Images = Labels\", train_count == len(list((dataset_path / 'train' / 'labels').glob('*.txt')))))\n",
    "    \n",
    "    for criteria, passed in checks:\n",
    "        status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
    "        print(f\"{status} - {criteria}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if all(c[1] for c in checks):\n",
    "        print(\"🎉 ALL CHECKS PASSED! Dataset ready for training!\")\n",
    "    else:\n",
    "        print(\"⚠️  Some checks failed - review the split\")\n",
    "else:\n",
    "    print(\"❌ No images found in split folders!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e5cc7",
   "metadata": {},
   "source": [
    "# 📋 STEP-BY-STEP EXECUTION GUIDE\n",
    "\n",
    "## Current Status Check:\n",
    "\n",
    "Anda sudah memiliki split yang ada:\n",
    "- **Train**: 428 images\n",
    "- **Valid**: 100 images\n",
    "- **Test**: 50 images\n",
    "\n",
    "## Option 1: Use Existing Split (RECOMMENDED) ✅\n",
    "\n",
    "Jika split sudah ada dan sesuai, **SKIP cells di atas** dan langsung ke training!\n",
    "\n",
    "Verifikasi dengan menjalankan cell \"Step 3: VERIFY SPLIT RESULTS\" saja.\n",
    "\n",
    "---\n",
    "\n",
    "## Option 2: Re-Split Dataset (Only if needed) ⚠️\n",
    "\n",
    "**HANYA jalankan jika**:\n",
    "- Split ratio tidak sesuai (harus 75-15-10)\n",
    "- Ada file yang corrupt/hilang\n",
    "- Mau menggunakan seed berbeda\n",
    "\n",
    "### Langkah-langkah Detail:\n",
    "\n",
    "#### 1️⃣ Install Dependencies\n",
    "```\n",
    "Run cell: \"Step 0: Install tqdm\"\n",
    "Expected: tqdm installed (~5 seconds)\n",
    "```\n",
    "\n",
    "#### 2️⃣ Load Split Function\n",
    "```\n",
    "Run cell: \"Step 1: OPTIMIZED Stratified Split Function\"\n",
    "Expected: Function loaded message\n",
    "Time: Instant\n",
    "```\n",
    "\n",
    "#### 3️⃣ Execute Split\n",
    "```\n",
    "Run cell: \"Step 2: RUN THE OPTIMIZED SPLIT\"\n",
    "Expected: \n",
    "  - Warning if split exists (choose y/n)\n",
    "  - Progress bars with tqdm\n",
    "  - Verification output\n",
    "Time: ~5-10 seconds (was 251 minutes!)\n",
    "```\n",
    "\n",
    "**What happens**:\n",
    "- Analyzes ball sizes (stratification)\n",
    "- Creates valid/test folders\n",
    "- **MOVES** (not copies) files from train/ to valid/ and test/\n",
    "- Updates data.yaml\n",
    "- Verifies final counts\n",
    "\n",
    "#### 4️⃣ Verify Results\n",
    "```\n",
    "Run cell: \"Step 3: VERIFY SPLIT RESULTS\"\n",
    "Expected:\n",
    "  - All checks PASS ✅\n",
    "  - Split percentages close to 75-15-10\n",
    "Time: ~2 seconds\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ IMPORTANT NOTES:\n",
    "\n",
    "### Performance Comparison:\n",
    "| Method | Time | Why? |\n",
    "|--------|------|------|\n",
    "| **Old (copy)** | 251 min ❌ | Copies files (read+write) |\n",
    "| **New (move)** | ~10 sec ⚡ | Moves files (rename only) |\n",
    "\n",
    "### What is MOVE vs COPY?\n",
    "\n",
    "**COPY** (old method):\n",
    "1. Read file from disk → Memory\n",
    "2. Write file to new location → Disk\n",
    "3. Keep original\n",
    "4. Result: 2× disk I/O, slow!\n",
    "\n",
    "**MOVE** (new method):\n",
    "1. Update directory pointer\n",
    "2. No data transfer!\n",
    "3. Result: Instant! ⚡\n",
    "\n",
    "### Why was it so slow (251 minutes)?\n",
    "\n",
    "Possible causes:\n",
    "1. ❌ External HDD (slow USB transfer)\n",
    "2. ❌ Windows Defender scanning each file\n",
    "3. ❌ Cell run multiple times\n",
    "4. ❌ Disk fragmentation\n",
    "\n",
    "### How to prevent slow splits:\n",
    "\n",
    "✅ Use SSD (not external HDD)\n",
    "✅ Temporarily disable antivirus\n",
    "✅ Use MOVE instead of COPY (this new method!)\n",
    "✅ Check if split exists before running\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps After Split:\n",
    "\n",
    "Once split is verified, proceed to:\n",
    "\n",
    "1. **Training Cell** → Train YOLOv8s model\n",
    "2. **Evaluation Cell** → Test on test set\n",
    "3. **Consistency Test** → Test across multiple videos\n",
    "\n",
    "Split is a **ONE-TIME operation** - don't run again unless needed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
